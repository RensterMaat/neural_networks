{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Tensor():\n",
    "    def __init__(self, value, requires_grad=True):\n",
    "        self.value = np.array(value, dtype=float)\n",
    "        self.requires_grad = requires_grad\n",
    "        self.a, self.b = None, None\n",
    "        self.dependencies = {self}\n",
    "\n",
    "        if requires_grad:\n",
    "            self.grad = np.zeros(self.value.shape)\n",
    "        \n",
    "    def backwards(self, queue=[], first=True):\n",
    "        if first:\n",
    "            self.grad = np.ones(self.value.shape)\n",
    "\n",
    "        if self.a is not None and self.a.requires_grad:\n",
    "            self.backpropagate_a()\n",
    "            # if not self.a in queue:\n",
    "            #     queue.append(self.a)\n",
    "            queue = self.add_to_queue(self.a, queue)\n",
    "\n",
    "        if self.b is not None and self.b.requires_grad:\n",
    "            self.backpropagate_b()\n",
    "            # if not self.b in queue:\n",
    "            #     queue.append(self.b)\n",
    "            queue = self.add_to_queue(self.b, queue)\n",
    "        \n",
    "        # print(self)\n",
    "        # print(queue)\n",
    "        # print(type(self), queue)\n",
    "\n",
    "        if queue:\n",
    "            queue[0].backwards(queue=queue[1:], first=False)\n",
    "\n",
    "    def add_to_queue(self, x, queue):\n",
    "        for ix, el in enumerate(queue):\n",
    "            if x == el:\n",
    "                return queue\n",
    "            if el in x.dependencies:\n",
    "                return queue[:ix] + [x] + queue[ix:]\n",
    "        return queue + [x]\n",
    "\n",
    "    def backpropagate_a(self):\n",
    "        pass\n",
    "\n",
    "    def backpropagate_b(self):\n",
    "        pass\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"({type(self)}, \\n value=\\n{self.value}, \\ngrad=\\n{self.grad if self.requires_grad else ''}\"\n",
    "\n",
    "    def __add__(self, b):\n",
    "        return Add(self, b)\n",
    "\n",
    "    def __sub__(self, b):\n",
    "        return Add(self, Neg(b))\n",
    "\n",
    "    def __mul__(self, b):\n",
    "        return Mul(self, b)\n",
    "\n",
    "    def __truediv__(self, b):\n",
    "        return Mul(self,  b ** Tensor(-1, requires_grad=False)) \n",
    "\n",
    "    def __pow__(self, b):\n",
    "        return Pow(self, b)\n",
    "\n",
    "    def __matmul__(self, b):\n",
    "        return Dot(self, b)\n",
    "\n",
    "    def __gt__(self, b):\n",
    "        return Tensor(self.value > b.value, requires_grad=False)\n",
    "\n",
    "\n",
    "class Add(Tensor):\n",
    "    def __init__(self, a, b):\n",
    "        super().__init__(a.value + b.value)\n",
    "        self.a, self.b = a, b\n",
    "        self.dependencies = {self}.union(a.dependencies).union(b.dependencies)\n",
    "\n",
    "    def backpropagate_a(self):\n",
    "        self.a.grad += self.grad\n",
    "\n",
    "    def backpropagate_b(self):\n",
    "        self.b.grad += self.grad\n",
    "\n",
    "\n",
    "class Neg(Tensor):\n",
    "    def __init__(self, a):\n",
    "        super().__init__(-a.value)\n",
    "        self.a = a\n",
    "        self.dependencies = {self}.union(a.dependencies)\n",
    "    \n",
    "    def backpropagate_a(self):\n",
    "        self.a.grad += -self.grad\n",
    "\n",
    "class Mul(Tensor):\n",
    "    def __init__(self, a, b):\n",
    "        super().__init__(a.value * b.value)\n",
    "        self.a, self.b = a, b\n",
    "        self.dependencies = {self}.union(a.dependencies).union(b.dependencies)\n",
    "\n",
    "    def backpropagate_a(self):\n",
    "        gradient = self.grad * self.b.value\n",
    "        if gradient.shape == self.a.grad.shape:\n",
    "            # print('adding to a...', type(self.a), gradient)\n",
    "            self.a.grad += gradient\n",
    "        else:\n",
    "            self.a.grad += gradient.sum(axis=0) \n",
    "\n",
    "    def backpropagate_b(self):\n",
    "        gradient = self.grad * self.a.value\n",
    "        if gradient.shape == self.b.grad.shape:\n",
    "            # print('adding to b...', gradient)\n",
    "            self.b.grad += gradient\n",
    "        else:\n",
    "            self.b.grad += gradient.sum(axis=0) \n",
    "\n",
    "class Dot(Tensor):\n",
    "    def __init__(self, a, b):\n",
    "        super().__init__(a.value @ b.value)\n",
    "        self.a, self.b = a, b\n",
    "        self.dependencies = {self}.union(a.dependencies).union(b.dependencies)\n",
    "\n",
    "    def backpropagate_a(self):\n",
    "        self.a.grad += self.grad @ self.b.value.T\n",
    "\n",
    "    def backpropagate_b(self):\n",
    "        self.b.grad += self.a.value.T @ self.grad\n",
    "\n",
    "\n",
    "class Pow(Tensor):\n",
    "    def __init__(self, a, b):\n",
    "        super().__init__(a.value ** b.value)\n",
    "        self.a, self.b = a, b\n",
    "        self.dependencies = {self}.union(a.dependencies).union(b.dependencies)\n",
    "\n",
    "    def backpropagate_a(self):\n",
    "        self.a.grad += self.b.value * self.a.value ** (self.b.value-1) * self.grad\n",
    "\n",
    "    def backpropagate_b(self):\n",
    "        self.b.grad += np.log(self.a.value) * self.a.value ** self.b.value * self.grad\n",
    "\n",
    "\n",
    "class Sum(Tensor):\n",
    "    def __init__(self, a):\n",
    "        super().__init__(sum(a.value))\n",
    "        self.a = a\n",
    "        self.dependencies = {self}.union(a.dependencies)\n",
    "\n",
    "    def backpropagate_a(self):\n",
    "        self.a.grad += self.grad * np.ones(self.a.value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real gradient: [-292.87406278]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-292.8746565746313"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "x = Tensor(np.random.randn(5,1), requires_grad=True)\n",
    "y = Tensor(np.random.randn(2,1), requires_grad=False)\n",
    "# u = Tensor(np.random.randn(5,1), requires_grad=True)\n",
    "\n",
    "m1 = Tensor(np.random.randn(4,5))\n",
    "m2 = Tensor(np.random.rand(6,4))\n",
    "m3 = Tensor(np.random.rand(2,6))\n",
    "\n",
    "\n",
    "\n",
    "def eval():\n",
    "    out = m1 @ x\n",
    "\n",
    "    out = out * (out > Tensor(0))\n",
    "    out = m2 @ out\n",
    "    out = out * (out > Tensor(0))\n",
    "    out = m3 @ out\n",
    "    # # out = Tensor(np.e, requires_grad=False) ** x\n",
    "    # out1 = x\n",
    "\n",
    "    # # z = out1 - Tensor(out.value.max(), requires_grad=False)\n",
    "    # out2 = Tensor(np.e, requires_grad=False) ** out1\n",
    "\n",
    "    # out3 = out2 / Sum(out2)\n",
    "\n",
    "    # out3 = out3 - y\n",
    "    # out3 = out3 ** Tensor(2, requires_grad=False)\n",
    "\n",
    "    # out4 = Sum(out3)\n",
    "\n",
    "    # return out1, out2, out3, out4\n",
    "\n",
    "    # out = Tensor(np.e, requires_grad=False) ** out\n",
    "    \n",
    "    out = out * Tensor(1, requires_grad=False)\n",
    "\n",
    "    out = (out) * Sum(out)\n",
    "\n",
    "    \n",
    "\n",
    "    out = (out - y) ** Tensor(2, requires_grad=False)\n",
    "\n",
    "    out = Sum(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "# x.grad = np.zeros(x.value.shape)\n",
    "\n",
    "out = eval()\n",
    "\n",
    "before = out.value\n",
    "out.backwards()\n",
    "\n",
    "delta = 0.0000000001\n",
    "nudge = np.zeros(m1.value.shape)\n",
    "nudge[1,1] = delta\n",
    "\n",
    "m1.value = m1.value + nudge\n",
    "\n",
    "out = eval()\n",
    "after = out.value\n",
    "\n",
    "print('Real gradient:', (after - before) / delta)\n",
    "\n",
    "# out.backwards()\n",
    "m1.grad[1,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<__main__.Sum at 0x20f2fc795d0>,\n",
       " <__main__.Tensor at 0x20f2fc79cc0>,\n",
       " <__main__.Mul at 0x20f2fc78520>,\n",
       " <__main__.Tensor at 0x20f180fef50>}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.a.b.dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Mul at 0x20f306c0910>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.add_to_queue(out.a, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55371241],\n",
       "       [0.55371241],\n",
       "       [0.55371241],\n",
       "       [0.55371241],\n",
       "       [0.55371241]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.a.a.b.value * out.a.a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out.a.a.b.value * out.a.a.grad).shape == out.a.a.a.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.60799791],\n",
       "       [-0.60799791],\n",
       "       [-0.60799791],\n",
       "       [-0.60799791],\n",
       "       [-0.60799791]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.a.a.a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1.5, 2, 3, 4]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queue = [1,2,3,4]\n",
    "el = 1.5\n",
    "\n",
    "queue[:1] + [el] + queue[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape () doesn't match the broadcast shape (3,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Rens\\repos\\neural_networks\\04_bfs.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39meval\u001b[39m()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m before \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mvalue\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m out\u001b[39m.\u001b[39;49mbackwards()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m delta \u001b[39m=\u001b[39m \u001b[39m0.0000000001\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m nudge \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(x\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;32mc:\\Users\\Rens\\repos\\neural_networks\\04_bfs.ipynb Cell 4\u001b[0m in \u001b[0;36mTensor.backwards\u001b[1;34m(self, queue, first)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         queue\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mif\u001b[39;00m queue:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     queue[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mbackwards(queue\u001b[39m=\u001b[39;49mqueue[\u001b[39m1\u001b[39;49m:], first\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\Rens\\repos\\neural_networks\\04_bfs.ipynb Cell 4\u001b[0m in \u001b[0;36mTensor.backwards\u001b[1;34m(self, queue, first)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         queue\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mif\u001b[39;00m queue:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     queue[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mbackwards(queue\u001b[39m=\u001b[39;49mqueue[\u001b[39m1\u001b[39;49m:], first\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "    \u001b[1;31m[... skipping similar frames: Tensor.backwards at line 28 (9 times)]\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Rens\\repos\\neural_networks\\04_bfs.ipynb Cell 4\u001b[0m in \u001b[0;36mTensor.backwards\u001b[1;34m(self, queue, first)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         queue\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mif\u001b[39;00m queue:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     queue[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mbackwards(queue\u001b[39m=\u001b[39;49mqueue[\u001b[39m1\u001b[39;49m:], first\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\Rens\\repos\\neural_networks\\04_bfs.ipynb Cell 4\u001b[0m in \u001b[0;36mTensor.backwards\u001b[1;34m(self, queue, first)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         queue\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackpropagate_b()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb \u001b[39min\u001b[39;00m queue:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         queue\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb)\n",
      "\u001b[1;32mc:\\Users\\Rens\\repos\\neural_networks\\04_bfs.ipynb Cell 4\u001b[0m in \u001b[0;36mAdd.backpropagate_b\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackpropagate_b\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/04_bfs.ipynb#W6sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb\u001b[39m.\u001b[39mgrad \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape () doesn't match the broadcast shape (3,)"
     ]
    }
   ],
   "source": [
    "x = Tensor([1,2,3], requires_grad=True)\n",
    "\n",
    "\n",
    "def eval():\n",
    "    # out = m1 @ x\n",
    "\n",
    "    # out = out * (out > Tensor(0))\n",
    "    # out = m2 @ out\n",
    "    # out = out * (out > Tensor(0))\n",
    "    # out = m3 @ out\n",
    "    out = x\n",
    "    z = out - Tensor(out.value.max(), requires_grad=False)\n",
    "    exps = Tensor(np.e, requires_grad=False) ** z\n",
    "    # out = x * y\n",
    "    out = exps * Sum(exps)\n",
    "    # out = out - y\n",
    "    # out = out ** Tensor(2, requires_grad=False)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "out = eval()\n",
    "\n",
    "before = out.value\n",
    "out.backwards()\n",
    "\n",
    "delta = 0.0000000001\n",
    "nudge = np.zeros(x.value.shape)\n",
    "nudge[1] = delta\n",
    "\n",
    "x.value = x.value + nudge\n",
    "\n",
    "out = eval()\n",
    "after = out.value\n",
    "\n",
    "print((after - before) / delta)\n",
    "\n",
    "\n",
    "x.grad[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.a.b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.61366234],\n",
       "       [-0.84146798],\n",
       "       [-1.24251437],\n",
       "       [ 0.27741298],\n",
       "       [ 0.44278722]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.999999999772853"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2,3])\n",
    "y = np.array([1,2,3])\n",
    "\n",
    "out = sum(sum(y) * x)\n",
    "before = out\n",
    "\n",
    "nudge = np.zeros(x.shape)\n",
    "delta = 0.00001\n",
    "nudge[0] = delta\n",
    "y = y + nudge\n",
    "\n",
    "out = sum(sum(y) * x)\n",
    "after = out\n",
    "\n",
    "(after - before) / delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200001.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".0001200001 / delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('conda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "febf68473bab6d033111f0bbfcdb01a5e43af7a2aa8031c222717ba26a5a8e95"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
