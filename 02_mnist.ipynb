{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import Tensor, Sum\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "ixx = np.arange(len(digits['data']))\n",
    "np.random.shuffle(ixx)\n",
    "\n",
    "for ix in ixx:\n",
    "    x = digits['data'][ix]\n",
    "    y = digits['target'][ix]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, input_dim, output_dim, bias=True):\n",
    "        self.bias = bias\n",
    "        self.m = Tensor(np.random.rand(output_dim, input_dim + int(bias)))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.bias:\n",
    "            x_with_extra_1 = np.ones(x.shape)\n",
    "\n",
    "\n",
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.m1 = Tensor(np.random.rand(32,64))\n",
    "        self.m2 = Tensor(np.random.rand(10,32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.m1 @ x\n",
    "        out = out * (out > Tensor(0))\n",
    "        out = self.m2 @ out\n",
    "\n",
    "        z = out - Tensor(out.value.max())\n",
    "        exps = Tensor(np.e, requires_grad=False) ** out\n",
    "\n",
    "        softmax = exps / Sum(exps)\n",
    "\n",
    "        return softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rens\\repos\\neural_networks\\src.py:74: RuntimeWarning: invalid value encountered in log\n",
      "  self.b.backwards(np.log(self.a.value) * self.a.value ** self.b.value * accumulated_gradient)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.15399165])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "x = (digits['data'][ix] / 16).reshape(-1, 1)\n",
    "y = np.zeros((10,1))\n",
    "y[digits['target'][ix],:] = 1\n",
    "\n",
    "x = Tensor(x, requires_grad=False)\n",
    "y = Tensor(y, requires_grad=False)\n",
    "\n",
    "out = model.forward(x)\n",
    "\n",
    "loss = Sum((out - y) ** Tensor(2, requires_grad=False))\n",
    "\n",
    "loss.backwards(np.ones(loss.value.shape))\n",
    "\n",
    "before = loss.value\n",
    "before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8930304753086191"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.m2.grad[3,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nudge = np.zeros(model.m2.value.shape)\n",
    "delta = 0.0001\n",
    "nudge[3,0] = delta\n",
    "\n",
    "model.m2.value = model.m2.value + nudge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15381316])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.forward(x)\n",
    "\n",
    "loss = Sum((out - y) ** Tensor(2, requires_grad=False))\n",
    "\n",
    "after = loss.value\n",
    "after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.78494995])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(after - before) / delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rens\\repos\\neural_networks\\src.py:74: RuntimeWarning: invalid value encountered in log\n",
      "  self.b.backwards(np.log(self.a.value) * self.a.value ** self.b.value * accumulated_gradient)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Rens\\repos\\neural_networks\\02_mnist.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/02_mnist.ipynb#W2sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m                 m\u001b[39m.\u001b[39mvalue \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39mvalue \u001b[39m-\u001b[39m lr \u001b[39m*\u001b[39m m\u001b[39m.\u001b[39mgrad\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/02_mnist.ipynb#W2sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m                 m\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(m\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/02_mnist.ipynb#W2sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(batch_loss)), batch_loss, alpha\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.signal import medfilt\n",
    "\n",
    "model = Model()\n",
    "\n",
    "lr = 0.1\n",
    "losses = [] \n",
    "batch_size = 64\n",
    "batch_loss = []\n",
    "\n",
    "for epoch in range(20):\n",
    "    for point, ix in enumerate(ixx):\n",
    "        x = (digits['data'][ix] / 16).reshape(-1, 1)\n",
    "        y = np.zeros((10,1))\n",
    "        y[digits['target'][ix],:] = 1\n",
    "        \n",
    "        x = Tensor(x, requires_grad=False)\n",
    "        y = Tensor(y, requires_grad=False)\n",
    "\n",
    "        y_hat = model.forward(x)\n",
    "\n",
    "\n",
    "        loss = Sum((y_hat - y) ** Tensor(2, requires_grad=False))\n",
    "        losses.append(loss.value[0])\n",
    "\n",
    "        loss.backwards(np.ones(loss.value.shape))\n",
    "\n",
    "        if point % batch_size == 0:\n",
    "            batch_loss.append(np.mean(losses[-64:]))\n",
    "            for m in [model.m1]:\n",
    "                m.value = m.value - lr * m.grad\n",
    "                m.grad = np.zeros(m.value.shape)\n",
    "\n",
    "    \n",
    "plt.plot(range(len(batch_loss)), batch_loss, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.40299656e-16],\n",
       "       [6.78785954e-17],\n",
       "       [3.57005342e-09],\n",
       "       [2.12554906e-21],\n",
       "       [4.83220073e-23],\n",
       "       [5.10446336e-17],\n",
       "       [4.70104924e-22],\n",
       "       [9.99999932e-01],\n",
       "       [2.41118438e-27],\n",
       "       [6.47508867e-08]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.25828803e-05],\n",
       "       [2.63058412e-04],\n",
       "       [4.87459122e-07],\n",
       "       [2.27067981e-03],\n",
       "       [7.92080467e-06],\n",
       "       [9.69198148e-01],\n",
       "       [3.59383007e-21],\n",
       "       [1.64374236e-03],\n",
       "       [4.41220866e-04],\n",
       "       [2.60821598e-02]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "[ 1.45742200e-33  2.04064664e-21 -4.41725722e-01  2.83721110e-03\n",
      "  6.04229144e-18  4.65374660e-17  1.34126485e-07  1.06978308e+02\n",
      "  2.57718467e+00  2.41725896e-13]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape () doesn't match the broadcast shape (10,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Rens\\repos\\neural_networks\\02_mnist.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Rens/repos/neural_networks/02_mnist.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m y_hat\u001b[39m.\u001b[39;49mbackwards(accum_grad)\n",
      "File \u001b[1;32mc:\\Users\\Rens\\repos\\neural_networks\\src.py:65\u001b[0m, in \u001b[0;36mMul.backwards\u001b[1;34m(self, accumulated_gradient)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackwards\u001b[39m(\u001b[39mself\u001b[39m, accumulated_gradient\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ma\u001b[39m.\u001b[39;49mbackwards(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mb\u001b[39m.\u001b[39;49mvalue \u001b[39m*\u001b[39;49m accumulated_gradient)\n\u001b[0;32m     66\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb\u001b[39m.\u001b[39mbackwards(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma\u001b[39m.\u001b[39mvalue \u001b[39m*\u001b[39m accumulated_gradient)\n",
      "File \u001b[1;32mc:\\Users\\Rens\\repos\\neural_networks\\src.py:75\u001b[0m, in \u001b[0;36mPow.backwards\u001b[1;34m(self, accumulated_gradient)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackwards\u001b[39m(\u001b[39mself\u001b[39m, accumulated_gradient\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 75\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ma\u001b[39m.\u001b[39;49mbackwards(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mb\u001b[39m.\u001b[39;49mvalue \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ma\u001b[39m.\u001b[39;49mvalue \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mb\u001b[39m.\u001b[39;49mvalue\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m*\u001b[39;49m accumulated_gradient)\n\u001b[0;32m     76\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb\u001b[39m.\u001b[39mbackwards(np\u001b[39m.\u001b[39mlog(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma\u001b[39m.\u001b[39mvalue) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma\u001b[39m.\u001b[39mvalue \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb\u001b[39m.\u001b[39mvalue \u001b[39m*\u001b[39m accumulated_gradient)\n",
      "File \u001b[1;32mc:\\Users\\Rens\\repos\\neural_networks\\src.py:16\u001b[0m, in \u001b[0;36mTensor.backwards\u001b[1;34m(self, accumulated_gradient)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     15\u001b[0m \u001b[39mprint\u001b[39m(accumulated_gradient)\n\u001b[1;32m---> 16\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m accumulated_gradient\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape () doesn't match the broadcast shape (10,)"
     ]
    }
   ],
   "source": [
    "y_hat.backwards(accum_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('conda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "febf68473bab6d033111f0bbfcdb01a5e43af7a2aa8031c222717ba26a5a8e95"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
